{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Set up OpenAI API credentials\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size):\n",
    "    # Load the WN18RR test set from a the txt file available here https://github.com/villmow/datasets_knowledge_embedding/raw/master/WN18RR/text/test.txt\n",
    "    wn18rr_test_set = pd.read_csv(\"test.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "    # Convert the test set to a list of triplets\n",
    "    test_triplets = [(triplet[0], triplet[1], triplet[2]) for triplet in wn18rr_test_set.values]\n",
    "\n",
    "    # Chunk the test triplets into batches of size 50\n",
    "    # batch_size = 50\n",
    "    num_batches = (len(test_triplets) + batch_size - 1) // batch_size\n",
    "    triplet_batches = [\n",
    "        test_triplets[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)\n",
    "    ]\n",
    "    return triplet_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong implementation in first step !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_batches = load_data(50)\n",
    "# Initialize lists to store responses\n",
    "responses = []\n",
    "generated_entities = []\n",
    "response_arrays = []\n",
    "\n",
    "# Open files in write mode\n",
    "file1 = open('results/responses.txt', 'w')\n",
    "file2 = open('results/generated_entities.txt', 'w')\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    prompt = 'Consider the relation prediction task, in the following lines first the entity and second the relation are separated by space. Give me all the missing entities in a single array in your answer:'\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]} {triplet[1]}'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses.append(response)\n",
    "    generated_entities.append(generated_entities)\n",
    "\n",
    "    file1.write(str(response)+ '\\n')\n",
    "    file2.write(str(generated_entity)+ '\\n')\n",
    "    time.sleep(20)\n",
    "\n",
    "# Close files\n",
    "file1.close()\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge_file = open('results/generated_entities.txt', 'r')\n",
    "results = []\n",
    "\n",
    "for line in ge_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results.append(array)\n",
    "    except: \n",
    "        results.append([])\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_batches = load_data(50)\n",
    "# Initialize lists to store responses\n",
    "responses2 = []\n",
    "generated_entities2 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    responses2_file = open('results/responses2.txt', 'a')\n",
    "    generated_entities2_file = open('results/generated_entities2.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"I have relation prediction task, consider these entities:\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]}'\n",
    "    prompt += \"\\nAnd also these relation respectively\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[1]}'\n",
    "    prompt += \"Find just missing entities (NOT THE RELATION OR OTHER FALSY WORDS!) respectively and give them to me in a single array with this format:\\nmissing entities:  ['', '', ...]\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses2.append(response)\n",
    "    generated_entities2.append(generated_entities)\n",
    "\n",
    "    responses2_file.write(str(response)+ '\\n')\n",
    "    generated_entities2_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    responses2_file.close()\n",
    "    generated_entities2_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge2_file = open('results/generated_entities2.txt', 'r')\n",
    "results2 = []\n",
    "\n",
    "for line in ge2_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results2.append(array)\n",
    "    except: \n",
    "        results2.append([])\n",
    "len(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count: 38', 'rate: 0.6031746031746031')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_batches = load_data(50)\n",
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(results2, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for a_list, r_list in zip(answers, results2):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        if r == a_list[i]:\n",
    "            correct += 1\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third prompt format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70 per prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_batches = load_data(70)\n",
    "# Initialize lists to store responses\n",
    "responses3 = []\n",
    "generated_entities3 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    responses3_file = open('results/responses3.txt', 'a')\n",
    "    generated_entities3_file = open('results/generated_entities3.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"I have relation prediction task, consider these entities:\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]}'\n",
    "    prompt += \"\\nAnd also these relation respectively\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[1]}'\n",
    "    prompt += \"Find just missing entities respectively and give them to me in a single array with this format:\\nmissing entities:  ['', '', ...]\\nNotice: 1.DO NOT GIVE ME THE RELATION OR OTHER FALSY WORDS IN YOUR RESPONSE!\\n2. THE LENGTH OF YOUR RESPONSE SHOULD BE: \"\n",
    "    prompt += str(len(triplet_batch))\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses3.append(response)\n",
    "    generated_entities3.append(generated_entities)\n",
    "\n",
    "    responses3_file.write(str(response)+ '\\n')\n",
    "    generated_entities3_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    responses3_file.close()\n",
    "    generated_entities3_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge3_file = open('results/generated_entities3.txt', 'r')\n",
    "results3 = []\n",
    "\n",
    "for line in ge3_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results3.append(array)\n",
    "    except: \n",
    "        results3.append([])\n",
    "len(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count: 37', 'rate: 0.8222222222222222')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_batches = load_data(70)\n",
    "\n",
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(results3, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for a_list, r_list in zip(answers, results3):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        if r == a_list[i]:\n",
    "            correct += 1\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 per prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_batches = load_data(50)\n",
    "# Initialize lists to store responses\n",
    "responses4 = []\n",
    "generated_entities4 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    responses4_file = open('results/responses4.txt', 'a')\n",
    "    generated_entities4_file = open('results/generated_entities4.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"I have relation prediction task, consider these entities:\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]}'\n",
    "    prompt += \"\\nAnd also these relation respectively\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[1]}'\n",
    "    prompt += \"Find just missing entities respectively and give them to me in a single array with this format:\\nmissing entities:  ['', '', ...]\\nNotice: 1.DO NOT GIVE ME THE RELATION OR OTHER FALSY WORDS IN YOUR RESPONSE!\\n2. THE LENGTH OF YOUR RESPONSE SHOULD BE: \"\n",
    "    prompt += str(len(triplet_batch))\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses4.append(response)\n",
    "    generated_entities4.append(generated_entities)\n",
    "\n",
    "    responses4_file.write(str(response)+ '\\n')\n",
    "    generated_entities4_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    responses4_file.close()\n",
    "    generated_entities4_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge4_file = open('results/generated_entities4.txt', 'r')\n",
    "results4 = []\n",
    "\n",
    "for line in ge4_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results4.append(array)\n",
    "    except: \n",
    "        results4.append([])\n",
    "len(results4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count: 45', 'rate: 0.7142857142857143')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_batches = load_data(50)\n",
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(results4, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(results4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for a_list, r_list in zip(answers, results4):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        if r == a_list[i]:\n",
    "            correct += 1\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('piciform_bird.n.01', '_hypernym', 'bird.n.01'),\n",
       " ('grocery_store.n.01', '_has_part', 'shelf.n.01'),\n",
       " ('transmit.v.04', '_derivationally_related_form', 'channelization.n.01')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_batches = load_data(50)\n",
    "samples = []\n",
    "for _ in range(3):\n",
    "    samples.append(triplet_batches[-1].pop())\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store responses\n",
    "responses5 = []\n",
    "generated_entities5 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    responses5_file = open('results/responses5.txt', 'a')\n",
    "    generated_entities5_file = open('results/generated_entities5.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"I have relation prediction task, consider these entities:\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]}'\n",
    "    prompt += \"\\nAnd also these relation respectively\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[1]}'\n",
    "    prompt += \"Find just missing entities respectively and give them to me in a single array with this format:\\nmissing entities:  ['', '', ...]\\nNotice: 1.DO NOT GIVE ME THE RELATION OR OTHER FALSY WORDS LIKE '' IN YOUR RESPONSE!\\n2. THE LENGTH OF YOUR RESPONSE SHOULD BE: \"\n",
    "    prompt += str(len(triplet_batch))\n",
    "    prompt += \"\\nI give you 3 samples, consider them:\\n\"\n",
    "    for i, s in enumerate(samples):\n",
    "        prompt += f'{str(i+1)}. entity: {s[0]}, relation: {s[1]}, missing entity(answer): {s[2]}\\n'\n",
    "    prompt += 'DO NOT give me these samples in your response!'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses5.append(response)\n",
    "    generated_entities5.append(generated_entities)\n",
    "\n",
    "    responses5_file.write(str(response)+ '\\n')\n",
    "    generated_entities5_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    responses5_file.close()\n",
    "    generated_entities5_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge5_file = open('results/generated_entities5.txt', 'r')\n",
    "results5 = []\n",
    "\n",
    "for line in ge5_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results5.append(array)\n",
    "    except: \n",
    "        results5.append([])\n",
    "len(results5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count: 48', 'rate: 0.7619047619047619')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(results5, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(results5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for a_list, r_list in zip(answers, results5):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        if r == a_list[i]:\n",
    "            correct += 1\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement with 10 suggestion for each item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firs try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_batches = load_data(20)\n",
    "# Initialize lists to store responses\n",
    "s_responses = []\n",
    "s_generated_entities = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    s_responses_file = open('results/s_responses.txt', 'a')\n",
    "    s_generated_entities_file = open('results/s_generated_entities.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"\"\"\n",
    "Consider the relation prediction task and I want you to behave as a relation prediction model according to the following rules:\n",
    "1. Data is provided in this format: <target_word> <relation>\\\\n\n",
    "2. You should  predict 10 sorted candidate words for each item\n",
    "3. Your answer should be a nested array that the length of the array is the count of given data, and each array contains 10 sorted candidate words\n",
    "4. \n",
    "5. The output MUST be a nested array with this format:\n",
    "  [[<candidate1>, <candidate2>, <candidate3>, <candidate4>, <candidate5>, <candidate6>, <candidate7>, <candidate8>, <candidate9>, <candidate10>], [<candidate1>, <candidate2>, <candidate3>, <candidate4>, <candidate5>, <candidate6>, <candidate7>, <candidate8>, <candidate9>, <candidate10>], ...]\n",
    "  Example: [['example1.n.01', 'example2.n.01', 'example3.n.01', 'example4.n.01', 'example5.n.01', 'example6.n.01', 'example7.n.01', 'example8.n.01', 'example9.n.01', 'example10.n.01']]\n",
    "6. Don't generate code, JUST give me the nested list\n",
    "7. Give output like this and no more explanation: result : <nested_list>\n",
    "\n",
    "data:\n",
    "\"\"\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]} {triplet[1]}'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1500,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    s_responses.append(response)\n",
    "    s_generated_entities.append(generated_entity)\n",
    "\n",
    "    s_responses_file.write(str(response)+ '\\n')\n",
    "    s_generated_entities_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    s_responses_file.close()\n",
    "    s_generated_entities_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [1:18:57<00:00, 30.18s/it]\n"
     ]
    }
   ],
   "source": [
    "triplet_batches = load_data(20)\n",
    "# Initialize lists to store responses\n",
    "s_responses2 = []\n",
    "s_generated_entities2 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    s_responses_file2 = open('results/s_responses2.txt', 'a')\n",
    "    s_generated_entities_file2 = open('results/s_generated_entities2.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = f\"\"\"\n",
    "Consider the relation prediction task and I want you to behave as a relation prediction model according to the following rules:\n",
    "1. Data is provided in this format: <target_word> <relation>\\\\n\n",
    "2. You should  predict 10 sorted candidate words for each item\n",
    "3. Your answer should be a nested array that the length of the array is the count of given data, and each array contains 10 sorted candidate words\n",
    "4. \n",
    "5. The output MUST be a nested array with this format:\n",
    "  [[<candidate1>, <candidate2>, <candidate3>, <candidate4>, <candidate5>, <candidate6>, <candidate7>, <candidate8>, <candidate9>, <candidate10>], [<candidate1>, <candidate2>, <candidate3>, <candidate4>, <candidate5>, <candidate6>, <candidate7>, <candidate8>, <candidate9>, <candidate10>], ...]\n",
    "  Example: [['example1.n.01', 'example2.n.01', 'example3.n.01', 'example4.n.01', 'example5.n.01', 'example6.n.01', 'example7.n.01', 'example8.n.01', 'example9.n.01', 'example10.n.01']]\n",
    "6. Don't generate code, JUST give me the nested list\n",
    "7. Give output like this and no more explanation: result : <nested_list>\n",
    "8. Don't use \\\\n or in your response\n",
    "9. Don't give me the example i gave you in rule number 5, give me candidate words!\n",
    "10. The out put should contains {len(triplet_batch)} arrays each contains 10 candidate, DO NOT make larger arrays and keep array format!\n",
    "\n",
    "data:\n",
    "\"\"\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]} {triplet[1]}'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=2000,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    s_responses2.append(response)\n",
    "    s_generated_entities2.append(generated_entity)\n",
    "\n",
    "    s_responses_file2.write(str(response)+ '\\n')\n",
    "    s_generated_entities_file2.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    s_responses_file2.close()\n",
    "    s_generated_entities_file2.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved data as list type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ge2_file = open('results/s_generated_entities2.txt', 'r')\n",
    "s_results2 = []\n",
    "\n",
    "for line in s_generated_entities2:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.rfind(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            array_str = '[]'\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        s_results2.append(array)\n",
    "    except: \n",
    "        s_results2.append([])\n",
    "len(s_results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate count and rate of bad output of API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count: 33', 'rate: 0.21019108280254778')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(s_results2, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(s_results2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate total reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.9543650793651"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reciprocal_rank = 0\n",
    "for a_list, r_list in zip(answers, s_results2):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        try:\n",
    "            true_rank = r.index(a_list[i]) + 1\n",
    "            reciprocal_rank = 1 / true_rank\n",
    "            # Add the reciprocal rank to the total\n",
    "            total_reciprocal_rank += reciprocal_rank\n",
    "        except:\n",
    "            total_reciprocal_rank += 0\n",
    "total_reciprocal_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get length of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3134"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn18rr_test_set = pd.read_csv(\"test.txt\", sep=\"\\t\", header=None)\n",
    "len(wn18rr_test_set.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0510384062154962"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reciprocal_rank/len(wn18rr_test_set.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change request setting and few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('piciform_bird.n.01', '_hypernym', 'bird.n.01'), 13)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_batches = load_data(30)\n",
    "print(len(triplet_batches[-1]))\n",
    "sample = triplet_batches[-1].pop()\n",
    "sample, len(triplet_batches[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [51:21<00:00, 29.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store responses\n",
    "s_responses3 = []\n",
    "s_generated_entities3 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    s_responses_file3 = open('results/s_responses3.txt', 'a')\n",
    "    s_generated_entities_file3 = open('results/s_generated_entities3.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = f\"\"\"\n",
    "Consider the relation prediction task and I want you to behave as a relation prediction model according to the following rules:\n",
    "1. Data is provided in this format: <target_word> <relation>\\\\n\n",
    "2. You should  predict 10 sorted candidate words for each item\n",
    "3. Your answer should be a nested array that the length of the array is the count of given data, and each array contains 10 sorted candidate words\n",
    "4. \n",
    "5. The output MUST be a nested array with this format:\n",
    "  [[<candidate1>, <candidate2>, <candidate3>, <candidate4>, <candidate5>, <candidate6>, <candidate7>, <candidate8>, <candidate9>, <candidate10>], [<candidate1>, <candidate2>, <candidate3>, <candidate4>, <candidate5>, <candidate6>, <candidate7>, <candidate8>, <candidate9>, <candidate10>], ...]\n",
    "  Example: [['example1.n.01', 'example2.n.01', 'example3.n.01', 'example4.n.01', 'example5.n.01', 'example6.n.01', 'example7.n.01', 'example8.n.01', 'example9.n.01', 'example10.n.01']]\n",
    "6. Don't generate code, JUST give me the nested list\n",
    "7. Give output like this and no more explanation: result : <nested_list>\n",
    "8. Don't use \\\\n or in your response\n",
    "9. Don't give me the example i gave you in rule number 5, give me candidate words!\n",
    "10. The out put should contains {len(triplet_batch)} arrays each contains 10 candidate, DO NOT make larger arrays and keep array format!\n",
    "11. An example of <target_word> <relation> <missed_entity> would be {sample[0]} {sample[1]} {sample[1]}. NOTE: it is just an example, don't give me this example as response!\n",
    "\n",
    "data:\n",
    "\"\"\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]} {triplet[1]}'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=2500,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    s_responses3.append(response)\n",
    "    s_generated_entities3.append(generated_entity)\n",
    "\n",
    "    s_responses_file3.write(str(response)+ '\\n')\n",
    "    s_generated_entities_file3.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    s_responses_file3.close()\n",
    "    s_generated_entities_file3.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved data as list type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ge3_file = open('results/s_generated_entities3.txt', 'r')\n",
    "s_results3 = []\n",
    "\n",
    "for line in s_generated_entities3:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.rfind(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            array_str = '[]'\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        s_results3.append(array)\n",
    "    except: \n",
    "        s_results3.append([])\n",
    "len(s_results3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate count and rate of bad output of API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count: 22', 'rate: 0.20952380952380953')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(s_results3, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(s_results3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate total reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.29047619047614"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reciprocal_rank = 0\n",
    "for a_list, r_list in zip(answers, s_results3):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        try:\n",
    "            true_rank = r.index(a_list[i]) + 1\n",
    "            reciprocal_rank = 1 / true_rank\n",
    "            # Add the reciprocal rank to the total\n",
    "            total_reciprocal_rank += reciprocal_rank\n",
    "        except:\n",
    "            total_reciprocal_rank += 0\n",
    "total_reciprocal_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get length of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3134"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn18rr_test_set = pd.read_csv(\"test.txt\", sep=\"\\t\", header=None)\n",
    "len(wn18rr_test_set.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04827392348132615"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reciprocal_rank/len(wn18rr_test_set.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat last exp. with 20 items per request (**THE BEST**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('piciform_bird.n.01', '_hypernym', 'bird.n.01'), 13)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_batches = load_data(20)\n",
    "print(len(triplet_batches[-1]))\n",
    "sample = triplet_batches[-1].pop()\n",
    "sample, len(triplet_batches[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [48:03<00:00, 24.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store responses\n",
    "s_responses4 = []\n",
    "s_generated_entities4 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches[40:]):\n",
    "    # Open files in append mode\n",
    "    s_responses_file4 = open('results/s_responses4.txt', 'a')\n",
    "    s_generated_entities_file4 = open('results/s_generated_entities4.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = f\"\"\"\n",
    "Consider the relation prediction task and I want you to behave as a relation prediction model according to the following rules:\n",
    "1. Data is provided in this format: <target_word> <relation>\\\\n\n",
    "2. You should  predict 10 sorted candidate words for each item\n",
    "3. Your answer should be a nested array that the length of the array is the count of given data, and each array contains 10 sorted candidate words\n",
    "4. \n",
    "5. The output MUST be a nested array with this format:\n",
    "  [[<candidate1>, <candidate2>, <candidate3>, <candidate4>, <candidate5>, <candidate6>, <candidate7>, <candidate8>, <candidate9>, <candidate10>], [<candidate1>, <candidate2>, <candidate3>, <candidate4>, <candidate5>, <candidate6>, <candidate7>, <candidate8>, <candidate9>, <candidate10>], ...]\n",
    "  Example: [['example1.n.01', 'example2.n.01', 'example3.n.01', 'example4.n.01', 'example5.n.01', 'example6.n.01', 'example7.n.01', 'example8.n.01', 'example9.n.01', 'example10.n.01']]\n",
    "6. Don't generate code, JUST give me the nested list\n",
    "7. Give output like this and no more explanation: result : <nested_list>\n",
    "8. Don't use \\\\n or in your response\n",
    "9. Don't give me the example i gave you in rule number 5, give me candidate words!\n",
    "10. The out put should contains {len(triplet_batch)} arrays each contains 10 candidate, DO NOT make larger arrays and keep array format!\n",
    "11. An example of <target_word> <relation> <missed_entity> would be {sample[0]} {sample[1]} {sample[1]}. NOTE: it is just an example, don't give me this example as response!\n",
    "\n",
    "data:\n",
    "\"\"\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]} {triplet[1]}'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=2500,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    s_responses4.append(response)\n",
    "    s_generated_entities4.append(generated_entity)\n",
    "\n",
    "    s_responses_file4.write(str(response)+ '\\n')\n",
    "    s_generated_entities_file4.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    s_responses_file4.close()\n",
    "    s_generated_entities_file4.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved data as list type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ge4_file = open('results/s_generated_entities4.txt', 'r')\n",
    "s_results4 = []\n",
    "\n",
    "for line in s_ge4_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.rfind(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            array_str = '[]'\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        s_results4.append(array)\n",
    "    except: \n",
    "        s_results4.append([])\n",
    "len(s_results4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate count and rate of bad output of API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count: 11', 'rate: 0.07006369426751592')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(s_results4, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(s_results4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate total reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.57420634920635"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reciprocal_rank = 0\n",
    "for a_list, r_list in zip(answers, s_results4):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        try:\n",
    "            true_rank = r.index(a_list[i]) + 1\n",
    "            reciprocal_rank = 1 / true_rank\n",
    "            # Add the reciprocal rank to the total\n",
    "            total_reciprocal_rank += reciprocal_rank\n",
    "        except:\n",
    "            total_reciprocal_rank += 0\n",
    "total_reciprocal_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get length of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3134"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn18rr_test_set = pd.read_csv(\"test.txt\", sep=\"\\t\", header=None)\n",
    "len(wn18rr_test_set.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058575049888068394"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reciprocal_rank/len(wn18rr_test_set.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
