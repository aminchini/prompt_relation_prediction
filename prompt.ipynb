{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Set up OpenAI API credentials\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WN18RR test set from a the txt file available here https://github.com/villmow/datasets_knowledge_embedding/raw/master/WN18RR/text/test.txt\n",
    "wn18rr_test_set = pd.read_csv(\"test.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "# Convert the test set to a list of triplets\n",
    "test_triplets = [(triplet[0], triplet[1], triplet[2]) for triplet in wn18rr_test_set.values]\n",
    "\n",
    "# Chunk the test triplets into batches of size 50\n",
    "batch_size = 50\n",
    "num_batches = (len(test_triplets) + batch_size - 1) // batch_size\n",
    "triplet_batches = [\n",
    "    test_triplets[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong implementation in first step !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store responses\n",
    "responses = []\n",
    "generated_entities = []\n",
    "response_arrays = []\n",
    "\n",
    "# Open files in write mode\n",
    "file1 = open('results/responses.txt', 'w')\n",
    "file2 = open('results/generated_entities.txt', 'w')\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    prompt = 'Consider the relation prediction task, in the following lines first the entity and second the relation are separated by space. Give me all the missing entities in a single array in your answer:'\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]} {triplet[1]}'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses.append(response)\n",
    "    generated_entities.append(generated_entities)\n",
    "\n",
    "    file1.write(str(response)+ '\\n')\n",
    "    file2.write(str(generated_entity)+ '\\n')\n",
    "    time.sleep(20)\n",
    "\n",
    "# Close files\n",
    "file1.close()\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_file = open('results/generated_entities.txt', 'r')\n",
    "results = []\n",
    "\n",
    "for line in ge_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results.append(array)\n",
    "    except: \n",
    "        results.append([])\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store responses\n",
    "responses2 = []\n",
    "generated_entities2 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    responses2_file = open('results/responses2.txt', 'a')\n",
    "    generated_entities2_file = open('results/generated_entities2.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"I have relation prediction task, consider these entities:\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]}'\n",
    "    prompt += \"\\nAnd also these relation respectively\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[1]}'\n",
    "    prompt += \"Find just missing entities (NOT THE RELATION OR OTHER FALSY WORDS!) respectively and give them to me in a single array with this format:\\nmissing entities:  ['', '', ...]\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses2.append(response)\n",
    "    generated_entities2.append(generated_entities)\n",
    "\n",
    "    responses2_file.write(str(response)+ '\\n')\n",
    "    generated_entities2_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    responses2_file.close()\n",
    "    generated_entities2_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_entities2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for ge in generated_entities2:\n",
    "    # Extract the array from the text\n",
    "    start_index = ge.find(\"[\")\n",
    "    end_index = ge.find(\"]\")\n",
    "    array_str = ge[start_index:end_index+1]\n",
    "    if end_index == -1:\n",
    "        array_str = '[]'\n",
    "    # Convert the string representation of the array to a Python list\n",
    "    array = ast.literal_eval(array_str)\n",
    "\n",
    "    # Append the extracted array\n",
    "    results.append(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results), len(triplet_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "len(answers[-1]), len(results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(results, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third prompt format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70 per prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WN18RR test set from a the txt file available here https://github.com/villmow/datasets_knowledge_embedding/raw/master/WN18RR/text/test.txt\n",
    "wn18rr_test_set = pd.read_csv(\"test.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "# Convert the test set to a list of triplets\n",
    "test_triplets = [(triplet[0], triplet[1], triplet[2]) for triplet in wn18rr_test_set.values]\n",
    "\n",
    "# Chunk the test triplets into batches of size 50\n",
    "batch_size = 70\n",
    "num_batches = (len(test_triplets) + batch_size - 1) // batch_size\n",
    "triplet_batches = [\n",
    "    test_triplets[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store responses\n",
    "responses3 = []\n",
    "generated_entities3 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    responses3_file = open('results/responses3.txt', 'a')\n",
    "    generated_entities3_file = open('results/generated_entities3.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"I have relation prediction task, consider these entities:\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]}'\n",
    "    prompt += \"\\nAnd also these relation respectively\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[1]}'\n",
    "    prompt += \"Find just missing entities respectively and give them to me in a single array with this format:\\nmissing entities:  ['', '', ...]\\nNotice: 1.DO NOT GIVE ME THE RELATION OR OTHER FALSY WORDS IN YOUR RESPONSE!\\n2. THE LENGTH OF YOUR RESPONSE SHOULD BE: \"\n",
    "    prompt += str(len(triplet_batch))\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses3.append(response)\n",
    "    generated_entities3.append(generated_entities)\n",
    "\n",
    "    responses3_file.write(str(response)+ '\\n')\n",
    "    generated_entities3_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    responses3_file.close()\n",
    "    generated_entities3_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge3_file = open('results/generated_entities3.txt', 'r')\n",
    "results3 = []\n",
    "\n",
    "for line in ge3_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results3.append(array)\n",
    "    except: \n",
    "        results3.append([])\n",
    "len(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(results3, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for a_list, r_list in zip(answers, results3):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        if r == a_list[i]:\n",
    "            correct += 1\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 per prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WN18RR test set from a the txt file available here https://github.com/villmow/datasets_knowledge_embedding/raw/master/WN18RR/text/test.txt\n",
    "wn18rr_test_set = pd.read_csv(\"test.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "# Convert the test set to a list of triplets\n",
    "test_triplets = [(triplet[0], triplet[1], triplet[2]) for triplet in wn18rr_test_set.values]\n",
    "\n",
    "# Chunk the test triplets into batches of size 50\n",
    "batch_size = 70\n",
    "num_batches = (len(test_triplets) + batch_size - 1) // batch_size\n",
    "triplet_batches = [\n",
    "    test_triplets[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store responses\n",
    "responses4 = []\n",
    "generated_entities4 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    responses4_file = open('results/responses4.txt', 'a')\n",
    "    generated_entities4_file = open('results/generated_entities4.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"I have relation prediction task, consider these entities:\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]}'\n",
    "    prompt += \"\\nAnd also these relation respectively\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[1]}'\n",
    "    prompt += \"Find just missing entities respectively and give them to me in a single array with this format:\\nmissing entities:  ['', '', ...]\\nNotice: 1.DO NOT GIVE ME THE RELATION OR OTHER FALSY WORDS IN YOUR RESPONSE!\\n2. THE LENGTH OF YOUR RESPONSE SHOULD BE: \"\n",
    "    prompt += str(len(triplet_batch))\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses4.append(response)\n",
    "    generated_entities4.append(generated_entities)\n",
    "\n",
    "    responses4_file.write(str(response)+ '\\n')\n",
    "    generated_entities4_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    responses4_file.close()\n",
    "    generated_entities4_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge4_file = open('results/generated_entities4.txt', 'r')\n",
    "results4 = []\n",
    "\n",
    "for line in ge4_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results4.append(array)\n",
    "    except: \n",
    "        results4.append([])\n",
    "len(results4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(results4, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(results4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for a_list, r_list in zip(answers, results4):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        if r == a_list[i]:\n",
    "            correct += 1\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('piciform_bird.n.01', '_hypernym', 'bird.n.01'),\n",
       " ('grocery_store.n.01', '_has_part', 'shelf.n.01'),\n",
       " ('transmit.v.04', '_derivationally_related_form', 'channelization.n.01')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the WN18RR test set from a the txt file available here https://github.com/villmow/datasets_knowledge_embedding/raw/master/WN18RR/text/test.txt\n",
    "wn18rr_test_set = pd.read_csv(\"test.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "# Convert the test set to a list of triplets\n",
    "test_triplets = [(triplet[0], triplet[1], triplet[2]) for triplet in wn18rr_test_set.values]\n",
    "\n",
    "# Chunk the test triplets into batches of size 50\n",
    "batch_size = 50\n",
    "num_batches = (len(test_triplets) + batch_size - 1) // batch_size\n",
    "triplet_batches = [\n",
    "    test_triplets[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)\n",
    "]\n",
    "\n",
    "samples = []\n",
    "for _ in range(3):\n",
    "    samples.append(triplet_batches[-1].pop())\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store responses\n",
    "responses5 = []\n",
    "generated_entities5 = []\n",
    "\n",
    "# Loop through the test triplets and make API calls to ChatGPT\n",
    "for triplet_batch in tqdm(triplet_batches):\n",
    "    # Open files in append mode\n",
    "    responses5_file = open('results/responses5.txt', 'a')\n",
    "    generated_entities5_file = open('results/generated_entities5.txt', 'a')\n",
    "\n",
    "    # prompt = \"Consider the relation prediction task, in the following lines the given entity comes first and the relation comes second after a space. Missing entities should have same format as given entities. Give me all the missing entities in a single array with this format:\\nmissing entities:  ['me1', 'me2', ...]\"\n",
    "    prompt = \"I have relation prediction task, consider these entities:\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[0]}'\n",
    "    prompt += \"\\nAnd also these relation respectively\"\n",
    "    for triplet in triplet_batch:\n",
    "        prompt += f'\\n{triplet[1]}'\n",
    "    prompt += \"Find just missing entities respectively and give them to me in a single array with this format:\\nmissing entities:  ['', '', ...]\\nNotice: 1.DO NOT GIVE ME THE RELATION OR OTHER FALSY WORDS LIKE '' IN YOUR RESPONSE!\\n2. THE LENGTH OF YOUR RESPONSE SHOULD BE: \"\n",
    "    prompt += str(len(triplet_batch))\n",
    "    prompt += \"\\nI give you 3 samples, consider them:\\n\"\n",
    "    for i, s in enumerate(samples):\n",
    "        prompt += f'{str(i+1)}. entity: {s[0]}, relation: {s[1]}, missing entity(answer): {s[2]}\\n'\n",
    "    prompt += 'DO NOT give me these samples in your response!'\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3100,  # Adjust the max tokens as per your requirements\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let ChatGPT decide when to stop the completion\n",
    "        temperature=0.6,  # Adjust the temperature as per your requirements\n",
    "    )\n",
    "    \n",
    "    # Extract the generated entity from the API response\n",
    "    generated_entity = response.choices[0].text.strip()\n",
    "    \n",
    "    responses5.append(response)\n",
    "    generated_entities5.append(generated_entities)\n",
    "\n",
    "    responses5_file.write(str(response)+ '\\n')\n",
    "    generated_entities5_file.write(str(generated_entity)+ '\\n')\n",
    "\n",
    "    # Close files\n",
    "    responses5_file.close()\n",
    "    generated_entities5_file.close()\n",
    "\n",
    "    # Time sleep for handling ChatGPT request rate limit\n",
    "    time.sleep(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge5_file = open('results/generated_entities5.txt', 'r')\n",
    "results5 = []\n",
    "\n",
    "for line in ge5_file:\n",
    "    # Extract the array from the text\n",
    "    try:\n",
    "        start_index = line.find(\"[\")\n",
    "        end_index = line.find(\"]\")\n",
    "        array_str = line[start_index:end_index+1]\n",
    "        if start_index == -1:\n",
    "            continue\n",
    "        if end_index == -1:\n",
    "            array_str = '[]'\n",
    "        # Convert the string representation of the array to a Python list\n",
    "        array = ast.literal_eval(array_str)\n",
    "\n",
    "        # Append the extracted array\n",
    "        results5.append(array)\n",
    "    except: \n",
    "        results5.append([])\n",
    "len(results5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('count: 48', 'rate: 0.7619047619047619')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = [[l for j,k,l in i] for i in triplet_batches]\n",
    "unbalance_count = 0\n",
    "for i, (r, a) in enumerate(zip(results5, answers)):\n",
    "    if not len(r) == len(a):\n",
    "        unbalance_count += 1\n",
    "'count: '+str(unbalance_count), 'rate: '+str(unbalance_count/len(results5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for a_list, r_list in zip(answers, results5):\n",
    "    for i, r in enumerate(r_list):\n",
    "        if i>len(a_list)-1:\n",
    "            break\n",
    "        if r == a_list[i]:\n",
    "            correct += 1\n",
    "correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
